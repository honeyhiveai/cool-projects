{
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# \"Logging Evaluations Runs\"\n## \"Start logging your pipeline comparisons to HoneyHive for benchmarking and sharing\"\nThis notebook is a companion to this [guide](https://docs.honeyhive.ai/quickstart/evaluations) <a target=\"_blank\" href=\"https://colab.research.google.com/github/honeyhiveai/honeyhive-cookbook/blob/master/docs/notebooks/quickstart_evaluations_sequential_runs.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n### Introduction\n\nIn the following example, we are going to walk through how to log your pipeline runs to HoneyHive for benchmarking and sharing. For a complete overview of evaluations in HoneyHive, you can refer to our [Evaluations Overview](/evaluation-overview) page.\n\n### Setup HoneyHive and get your API key\n\nIf you haven't already done so, then the first thing you will need to do is <a href=\"/projects#create-a-new-project\">create a HoneyHive project</a>.\n\nAfter creating the project, you can find your API key in the [Settings](https://app.honeyhive.ai/settings/account) page under Account.\n\nOnce you have created a HoneyHive project and got your API keys, you can now start tracing your custom pipeline.\n\n### Install the SDK & authenticate\n\nWe currently support a native Python SDK. For other languages, we encourage using HTTP request libraries to send requests.\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "!pip install honeyhive -q\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n\nAuthenticate `honeyhive` and all the packages required to execute your pipeline. Make sure to import `time` for tracking latency.\n\nFor this quickstart tutorial, we will do a simple evaluation comparing 2 different `gpt-3.5-turbo` variants.\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "import honeyhive\nimport openai\nimport time\n\n# import any other vector databases, APIs and other model providers you might need\nhoneyhive.api_key = \"HONEYHIVE_API_KEY\"\nopenai.api_key = \"OPENAI_API_KEY\"\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n\n### Initialize Evaluation\n\nConfigure your evaluation by pointing it to the project you have in HoneyHive. `HoneyHiveEvaluation` accepts the following parameters:\n\n1. `project`: a necessary field which decides which project to log the evaluation to\n2. `name`: a necessary field which sets the name for the evaluation\n3. `description`: an optional field to describe what kind of evaluation you are attempting\n4. `dataset_name`: an optional field to set a name for the dataset you ran the evaluation over\n5. `metrics`: an optional field to specify which HoneyHive configured metrics you want to run\n\nThis tutorial's evaluation is set to try different `max_tokens` settings and compute an AI feedback function `Conciseness` and a custom metric `Number of Characters`.\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "honeyhive_eval = HoneyHiveEvaluation(\n    project=\"Email Writer App\",\n    name=\"Max Tokens Comparison\",\n    description=\"Finding best max tokens for OpenAI chat models\",\n    dataset_name=\"Test\",\n    metrics=[\"Conciseness\", \"Number of Characters\"] \n)\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n<br/>\n<Note>**Note on Metrics:** In this function, you can only define metrics that are configured for your project within HoneyHive. These metrics will automatically be computed across your evaluation harness upon ingestion. Learn more about how to define metrics and guardrails in HoneyHive.</Note>\n\n<Card title=\"Metrics\" icon=\"rectangle-terminal\" href=\"/metrics-custom\">\nHow to create a metric in HoneyHive via the UI\n</Card>\n\n<br/>\n\n### Prepare A Dataset & Configurations\n\nNow that the evaluation is configured, we can set up our offline evaluation. Begin by fetching a dataset to evaluate over.\n\n<Note>Try to pick data as close to your production distribution as possible</Note>\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "dataset = [\n    {\"topic\": \"Test\", \"tone\": \"test\"},\n    {\"topic\": \"AI\", \"tone\": \"neutral\"}\n]\n\n# in case you have a saved dataset in HoneyHive\nfrom honeyhive.sdk.datasets import get_dataset\ndataset = get_dataset(\"Email Writer Samples\")\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n\nFor evaluation configurations, we can optionally set any arbitrary field. However, there are some reserved fields for the platform:\n\n1. `name`, `version`: these are the reserved fields to render the display name for this configuration\n2. `model`, `provider`, `hyperparameters`: these are reserved to render the correct model in the UI to allow others to fork their variants\n3. `prompt_template`, `chat`: similar to the previous fields, these are needed as well for consistent forking logic\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "config =  {\n      \"name\": \"max_tokens_100\",\n      \"model\": \"gpt-3.5-turbo\",\n      \"provider\": \"openai\",\n      \"hyperparameters\": {\n        \"temperature\": 0.5, \n        \"max_tokens\": 100\n      },\n      \"chat\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant who helps people write emails.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Topic: {{topic}}\\n\\nTone: {{tone}}.\"\n        }\n      ]\n}\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n\n### Run and Log Evaluation\n\nWe log our evaluation runs via the `log_run` function on the `HoneyHiveEvaluation` object. `log_run` accepts one input-output pair at a time.\n\n`log_run` accepts 4 parameters:\n\n1. `config`: a necessary field specifying the configuration dictionary for that run\n2. `input`: a necessary field specifying the datapoint dictionary for that run\n3. `completion`: a necessary field specifying the final string response at the end of the pipeline\n4. `metrics`: an optional field specifying any recorded metrics of interest for the run (ex. cost, latency, etc)\n\n<Note>Parallelize your evaluation runs whenever possible! This will help you save 10x on time.</Note>\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "for data in dataset:\n    data_run = []\n    messages = honeyhive.utils.fill_chat_template(config[\"chat\"], data)\n\n    start = time.time()\n    openai_response = openai.ChatCompletion.create(\n        model=config[\"model\"],\n        messages=messages,\n        **config[\"hyperparameters\"]\n    )\n    end = time.time()\n\n    # log a particular run via log_run\n    honeyhive_eval.log_run(\n        config=config, \n        input=data,\n        completion=openai_response.choices[0].message.content,\n        metrics={\n            \"cost\": honeyhive.utils.calculate_openai_cost(\n                config[\"model\"], openai_response.usage\n            ),\n            \"latency\": (end - start) * 1000,\n            \"response_length\": openai_response.usage[\"completion_tokens\"],\n            **openai_response[\"usage\"]\n        }\n    )\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n\n### Run more configurations\n\nNow we can tweak the config and re-run our pipeline while logging the new runs via `log_run`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "config =  {\n      # same configuration as above here\n      \"hyperparameters\": {\"temperature\": 0.5, \"max_tokens\": 400},\n}\n\n# identical Evaluation Run code as above\nfor data in dataset:\n...\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n\n\n### Log Comments & Publish\n\nFinally, we can log any comments from what we have seen in the logs or while iterating on the pipeline and publish the evaluation.\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "honeyhive_eval.log_comment(\"Results are decent\")\nhoneyhive_eval.finish()\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    }
}