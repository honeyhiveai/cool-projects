{
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n# \"Logging LLM Requests\"\n## \"Get started by logging your LLM completion requests with HoneyHive.\"\nThis notebook is a companion to this [guide](https://docs.honeyhive.ai/quickstart/completions) <a target=\"_blank\" href=\"https://colab.research.google.com/github/honeyhiveai/honeyhive-cookbook/blob/master/docs/notebooks/quickstart_completions_open_source.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n\n### Introduction\n\nOur Python SDK allows you to trace your log individual LLM requests as well as full pipeline traces. This allows you to monitor your LLM's performance and log user feedback and ground truth labels associated with it.\n\nFor an in-depth overview of how our logging data is structured, please see our [Logging Overview](/logging-overview) page.\n\n### Get API key\n\nAfter signing up on the app, you can find your API key in the [Settings](https://app.honeyhive.ai/settings/account) page under Account.\n\n### Install the SDK\n\nWe currently support a native Python SDK. For other languages, we encourage using HTTP request libraries to send requests.\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "!pip install honeyhive -q\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n\n### Capture relevant details on your completion requests\n\nThis method allows you to log any arbitrary LLM requests on the client-side **without proxying your requests via HoneyHive servers**. Using this method, evaluation metrics such as custom metrics and AI feedback functions will be automatically computed based on the metrics you've defined and enabled in the [Metrics](https://app.honeyhive.ai/metrics) page. Learn more about defining evaluation metrics [here](/evaluation).\n\nLet's start by running an OpenAI Chat Completion request and calculate a basic metric like latency.\n\n<Note>We're using `OpenAI`, `Anthropic` and `Hugging Face` models in this guide to simply demonstrate how to log requests with HoneyHive. You can alternatively use the SDK to log any arbitrary model completion requests across other model providers such as `Cohere`, `AI21 Labs`, or your own custom, self-hosted models.</Note>\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "import honeyhive\nfrom honeyhive.sdk.utils import fill_template\nfrom transformers import AutoTokenizer # & appropriate model imports\nimport time\n\nhoneyhive.api_key = \"HONEYHIVE_API_KEY\"\n\nhf_model_path = \"dummy/model\"\n\ntokenizer = AutoTokenizer.from_pretrained(hf_model_path)\n# using the huggingface model import\n# model = \n\nUSER_TEMPLATE = f\"{HUMAN_PROMPT} Write me an email on {{topic}} in a {{tone}} tone.{AI_PROMPT}\"\nuser_inputs = {\n    \"topic\": \"AI Services\",\n    \"tone\": \"Friendly\"\n}\n#\"Write an email on AI Services in a Friendly tone.\"\nuser_message = fill_template(USER_TEMPLATE, user_inputs)\n\nstart = time.perf_counter()\n\n# tokenize the inputs\n# model_inputs = tokenizer(user_message)\n\n# generate model completion\n# generated_ids = model.generate(**model_inputs)\n\n# decode the tokens\n# decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nend = time.perf_counter()\n\nrequest_latency = (end - start)*1000\ngeneration = decoded_output\ntoken_usage = {\n    \"completion_tokens\": len(generated_ids),\n    \"prompt_tokens\": len(model_inputs),\n    \"total_tokens\": len(generated_ids) + len(model_inputs)\n}\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n\n### Log your completion request\n\nNow that you've run the request, let's try logging the request and some user metadata in HoneyHive. \n\n<Note>Adding a `session_id` field on the `metadata` field will enable session tracking on completions</Note>\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "\ngpu_cost_per_ms = 0.0000006\nresponse = honeyhive.generations.log(\n    project=\"Sandbox - Email Writer\",\n    source=\"staging\",\n    model=model_path,\n    hyperparameters={\n        # any additional arguments used when generating model completion\n    },\n    prompt_template=USER_TEMPLATE,\n    inputs=user_inputs,\n    generation=generation,\n    metadata={\n        \"session_id\": session_id  # Optionally specify a session id to track related completions\n    },\n    usage=token_usage,\n    latency=request_latency,\n    cost=request_latency * gpu_cost_per_ms,\n    user_properties={\n        \"user_device\": \"Macbook Pro\",\n        \"user_Id\": \"92739527492\",\n        \"user_country\": \"United States\",\n        \"user_subscriptiontier\": \"Enterprise\",\n        \"user_tenantID\": \"Acme Inc.\"\n    }\n)\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n<br/>\n<Note>Using this method, you will not be able to use our Prompt CI/CD capabilities within the platform or calculate cost and latency metrics automatically. In order to update prompts, you will need to manually update the prompt, model provider and hyperparamater settings in your codebase when deploying new variants to production.</Note>\n\n### Log user feedback and ground truth labels\n\nNow that you've logged a request in HoneyHive, let's try logging user feedback and ground truth labels associated with that completion. \n\nUsing the `generation_id` that is returned, you can send arbitrary feedback to HoneyHive using the `feedback` endpoint.\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "from honeyhive.sdk.feedback import generation_feedback\ngeneration_feedback(\n    project=\"Sandbox - Email Writer\",\n    generation_id=response.generation_id,\n    ground_truth=\"INSERT_GROUND_TRUTH_LABEL\",\n    feedback_json={\n        \"provided\": True,\n        \"accepted\": False,\n        \"edited\": True\n    }\n)\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n\n### [Optional] Proxy requests via HoneyHive\n\nAlternatively, you can automatically call the current deployed prompt-model configuration within a specified project without specifying all the parameters. Using this method, we automatically route your requests to the model-prompt configuration that is currently active within the platform and capture some basic metrics like cost and latency.\n\nMore documentation can be found on our [saved prompt generations API page](/api-reference/generations/post_saved).\n\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "source": [
                "import honeyhive\n\nhoneyhive.api_key = \"HONEYHIVE_API_KEY\"\nhoneyhive.openai_api_key = \"OPENAI_API_KEY\"\n\nresponse = honeyhive.generations.generate(\n    project=\"Sandbox - Email Writer\",\n    source=\"staging\",\n    input={\n        \"topic\": \"Model evaluation for companies using GPT-4\",\n        \"tone\": \"friendly\"\n    },\n)\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "hhai"
                    ]
                }
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    }
}